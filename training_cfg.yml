experiment:
  name: vmax_mse_v2
  # Number of past time steps given as input
  past_steps: 4
  # Number of time steps to predict at once
  future_steps: 4
  # Size of the input and output patches, in pixels (resolution=0.07Â°)
  patch_size: 128
  # Id of a W&B run from which to download the model to fine-tune it
  # If left to None, a new model we trained from scratch
  use-pre-trained-id:
tasks:
  # Parameters specific to each task
  # distribution:
  # - "deterministic" (MSE loss)
  # - "normal"
  # - "qc" (quantiles composite)
  vmax:
    output_variables: ['INTENSITY']
    distribution: deterministic
  # mslp:
  # output_variables: ['MSLP']
  # distribution: deterministic
  # r34_avg:
  # output_variables: ['R35_4qAVG']
  # distribution: deterministic
  # location:
  # output_variables: ['LAT', 'LON']
  # distribution: deterministic
training_settings:
  epochs: 100
  batch_size: 128
  # Trainer precision, usually "bf16-mixed" or "32-true"
  precision: 32-true
  # Weight decay
  weight_decay: 0
  # Whether to enable data augmentation
  data_augmentation: true
  # Sampling weights
  sampling_weights: false
  # Whether to use a weighted loss for extreme events
  use_weighted_loss: false
  # Learning-related params
  initial_lr: 0.001
  final_lr: 1.0e-7
  beta1: 0.9
  beta2: 0.999
  # If fine-tuning, freeze the encoder
  freeze_encoder:
  # Optionally, specify a single task to train on
  trained_task:
model_hyperparameters:
  # What type of base block to use (either 'cbam' or 'conv')
  base_block: conv
  # Number of convolutional blocks (Each block has 2 Conv layers)
  encoder_depth: 7
  # Number of channels in the first hidden layer
  encoder_channels: 8
  # Reduction factor in the Common Linear Module
  clm_reduction_factor: 8

