experiment:
  name: det_era5_env
  group: test
  # Whether to train on the training set and evaluate on the val set (false)
  # or train on the full set and evaluate on the test set (true).
  use_full_dataset: true
  # Number of past time steps given as input
  past_steps: 4
  # List of the target time steps, as indices, e.g.
  # [0, 2] means +0h and +12h.
  target_steps: [1, 2, 3, 4]
  # Size of the input and output patches, in pixels (resolution=0.07Â°)
  patch_size: 128
  # Which channels to use as input
  input_channels: ['IR']
  # Which contextual variables to include: (must be at least [])
  # LAT, LON, HOUR_COS, HOUR_SIN
  context_variables: ['LAT', 'LON', 'HOUR_COS', 'HOUR_SIN']
  # Optionally: indicate a minimum SSHS category to train on (from -1 to 5)
  # If None, no filtering is applied (equivalent to -1).
  train_min_category: -1.
  # Id of a W&B run from which to download the model to fine-tune it
  # If left to None, a new model we trained from scratch
  use-pre-trained-id:
tasks:
  # Parameters specific to each task
  # distribution:
  # - "deterministic" (MSE loss)
  # - "normal"
  # - "qc" (quantiles composite)
  # - "multivariate_normal"
  # predict_residuals: true / false
  #   Whether to predict Y_0 and the residuals (Y_t - Y_0)
  #   (if true), or to just directly predict the target Y_t.
  # weight: float
  #   Weight of the task in the final loss.
  #   The weights are normalized internally, and so don't need to sum to 1.
  vmax:
    output_variables: ['INTENSITY']
    distribution: deterministic
    predict_residuals: false
    weight: 1
  # mslp:
  # output_variables: ['MSLP']
  # distribution: deterministic
  # predict_residuals: false
  # r34_avg:
  # output_variables: ['R35_4qAVG']
  # distribution: deterministic
  # location:
  # output_variables: ['LAT', 'LON']
  # distribution: deterministic
training_settings:
  epochs: 100
  batch_size: 128
  # Trainer precision, usually "bf16-mixed" or "32-true"
  precision: 16-mixed
  # Number of workers for the data loading
  num_workers: 9
  # Weight decay
  weight_decay: 0.05
  # Whether to enable data augmentation
  data_augmentation: true
  # Sampling weights
  sampling_weights: false
  # Whether to use a weighted loss for extreme events
  use_weighted_loss: true
  # Exponential tilting parameter of the loss (based on TERM)
  # 0 or None will be interpreted as no tilting
  loss_tilting: 0
  # Learning-related params
  initial_lr: 2.8e-4
  final_lr: 1.0e-8
  beta1: 0.873
  beta2: 0.977
  # Weight of the residual loss
  # 0. means no residual loss, 1. means no location loss
  residual_loss_weight: 0.05
  # If fine-tuning, freeze the encoder
  freeze_encoder:
  # Optionally, specify a single task to train on
  trained_task:
model_hyperparameters:
  spatial_encoder:
    # What type of base block to use (either 'cbam' or 'conv')
    base_block: conv
    n_blocks: 5
    base_channels: 8
  temporal_encoder:
    reduction_factor: 16
    n_blocks: 3
  prediction_head:
    reduction_factor: 16
